{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08bd257c-45aa-430d-8b0d-e73decad85f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "spark.conf.set(\"fs.azure.account.auth.type.storageansh.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.storageansh.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.storageansh.dfs.core.windows.net\", \"73b19484-4367-47ae-9797-29c5c0eddafc\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.storageansh.dfs.core.windows.net\", \"GT98Q~8OthIGccstIBhrF1IXU3Tz1df6HJgXGbVQ\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.storageansh.dfs.core.windows.net\", \"https://login.microsoftonline.com/55cca473-f9ef-4ccb-ba85-dec3acbc0a2c/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60b01e0b-844d-4bef-a889-cefe8c705a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1225 - Report Contiguous Dates\n",
    "\n",
    "https://leetcode.ca/2019-04-08-1225-Report-Contiguous-Dates/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "759a7278-bea3-4e3c-9ce2-a334420611d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException: [TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create table or view `leet`.`failed` because it already exists.\n",
       "Choose a different name, drop the existing object, add the IF NOT EXISTS clause to tolerate pre-existing objects, add the OR REPLACE clause to replace the existing materialized view, or add the OR REFRESH clause to refresh the existing streaming table. SQLSTATE: 42P07\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.tableAlreadyExistsError(QueryCompilationErrors.scala:1919)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:66)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandWithAetherOff(SparkPlan.scala:181)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:192)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:387)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:387)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:199)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:387)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:460)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:745)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:335)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1281)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:206)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:682)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:383)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1127)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:379)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:329)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:377)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:431)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:426)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:505)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:505)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:379)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:375)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:481)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:426)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:426)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:288)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:285)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:383)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:132)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1281)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1288)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1288)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:123)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1059)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1281)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1023)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1083)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:312)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:412)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:435)\n",
       "\tat scala.collection.immutable.List.map(List.scala:293)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:430)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:40)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$35(DriverLocal.scala:1203)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$29(DriverLocal.scala:1194)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:104)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:104)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1121)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1568)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:788)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:1062)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:1051)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:1097)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:92)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:92)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:92)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:1097)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:785)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:892)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:657)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:92)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:647)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:570)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:354)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "[TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create table or view `leet`.`failed` because it already exists.\nChoose a different name, drop the existing object, add the IF NOT EXISTS clause to tolerate pre-existing objects, add the OR REPLACE clause to replace the existing materialized view, or add the OR REFRESH clause to refresh the existing streaming table. SQLSTATE: 42P07"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "TABLE_OR_VIEW_ALREADY_EXISTS",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42P07",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException: [TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create table or view `leet`.`failed` because it already exists.\nChoose a different name, drop the existing object, add the IF NOT EXISTS clause to tolerate pre-existing objects, add the OR REPLACE clause to replace the existing materialized view, or add the OR REFRESH clause to refresh the existing streaming table. SQLSTATE: 42P07\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.tableAlreadyExistsError(QueryCompilationErrors.scala:1919)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:66)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandWithAetherOff(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:192)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:387)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:387)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:199)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:387)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:460)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:745)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:335)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1281)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:206)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:682)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:383)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1127)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:379)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:329)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:377)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:431)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:426)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:505)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:505)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:40)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:379)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:375)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:40)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:40)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:481)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:426)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:426)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:288)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:285)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:383)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:132)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1281)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1288)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1288)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:123)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1059)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1281)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1023)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1083)\n\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:312)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:412)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:435)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:430)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:40)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$35(DriverLocal.scala:1203)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$29(DriverLocal.scala:1194)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:104)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:104)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1121)\n\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1568)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:788)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:1062)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:1051)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:1097)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:92)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:92)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:92)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:1097)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:785)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:892)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:657)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:92)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:647)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:570)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:354)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "CREATE TABLE leet.failed (\n",
    "    fail_date DATE\n",
    ")\n",
    "USING DELTA;\n",
    "\n",
    "-- Insert data into the Failed table\n",
    "INSERT INTO leet.failed VALUES\n",
    "    ('2018-12-28'),\n",
    "    ('2018-12-29'),\n",
    "    ('2019-01-04'),\n",
    "    ('2019-01-05');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bd3b21e-d323-47ec-b4f7-399bf99e47b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>6</td><td>6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         6,
         6
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "CREATE TABLE leet.succeeded (\n",
    "    success_date DATE\n",
    ")\n",
    "USING DELTA;\n",
    "\n",
    "-- Insert data into the Succeeded table\n",
    "INSERT INTO leet.succeeded VALUES\n",
    "    ('2018-12-30'),\n",
    "    ('2018-12-31'),\n",
    "    ('2019-01-01'),\n",
    "    ('2019-01-02'),\n",
    "    ('2019-01-03'),\n",
    "    ('2019-01-06');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25b9a6fc-078b-4f53-a682-c081b7eca298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dt</th><th>st</th></tr></thead><tbody><tr><td>2018-12-28</td><td>failed</td></tr><tr><td>2018-12-29</td><td>failed</td></tr><tr><td>2019-01-04</td><td>failed</td></tr><tr><td>2019-01-05</td><td>failed</td></tr><tr><td>2018-12-30</td><td>succeeded</td></tr><tr><td>2018-12-31</td><td>succeeded</td></tr><tr><td>2019-01-01</td><td>succeeded</td></tr><tr><td>2019-01-02</td><td>succeeded</td></tr><tr><td>2019-01-03</td><td>succeeded</td></tr><tr><td>2019-01-06</td><td>succeeded</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2018-12-28",
         "failed"
        ],
        [
         "2018-12-29",
         "failed"
        ],
        [
         "2019-01-04",
         "failed"
        ],
        [
         "2019-01-05",
         "failed"
        ],
        [
         "2018-12-30",
         "succeeded"
        ],
        [
         "2018-12-31",
         "succeeded"
        ],
        [
         "2019-01-01",
         "succeeded"
        ],
        [
         "2019-01-02",
         "succeeded"
        ],
        [
         "2019-01-03",
         "succeeded"
        ],
        [
         "2019-01-06",
         "succeeded"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dt",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "st",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with c1 as (\n",
    "  select fail_date as dt, 'failed' as st\n",
    "  from leet.failed\n",
    "  union all\n",
    "  select success_date as dt, 'succeeded' as st\n",
    "  from leet.succeeded\n",
    ")\n",
    "select * from c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b41884-24c3-459a-ae53-722c78249f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>period_state</th><th>start_date</th><th>end_date</th></tr></thead><tbody><tr><td>failed</td><td>2018-12-28</td><td>2018-12-29</td></tr><tr><td>succeeded</td><td>2018-12-30</td><td>2019-01-03</td></tr><tr><td>failed</td><td>2019-01-04</td><td>2019-01-05</td></tr><tr><td>succeeded</td><td>2019-01-06</td><td>2019-01-06</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "failed",
         "2018-12-28",
         "2018-12-29"
        ],
        [
         "succeeded",
         "2018-12-30",
         "2019-01-03"
        ],
        [
         "failed",
         "2019-01-04",
         "2019-01-05"
        ],
        [
         "succeeded",
         "2019-01-06",
         "2019-01-06"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "period_state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "start_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "end_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "WITH all_tasks AS (\n",
    "    SELECT \n",
    "        fail_date AS task_date, \n",
    "        'failed' AS period_state\n",
    "    FROM leet.failed\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        success_date AS task_date, \n",
    "        'succeeded' AS period_state\n",
    "    FROM leet.succeeded\n",
    "),\n",
    "grouped_tasks AS (\n",
    "    SELECT \n",
    "        task_date,\n",
    "        period_state,\n",
    "        DATE_SUB(task_date, ROW_NUMBER() OVER (PARTITION BY period_state ORDER BY task_date)) AS grp\n",
    "    FROM all_tasks\n",
    ")\n",
    "SELECT \n",
    "    period_state,\n",
    "    MIN(task_date) AS start_date,\n",
    "    MAX(task_date) AS end_date\n",
    "FROM grouped_tasks\n",
    "GROUP BY \n",
    "    period_state, grp\n",
    "ORDER BY \n",
    "    start_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86dcb82f-d0d6-48a8-b860-f5877bd7c937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>task_date</th><th>period_state</th><th>rr</th><th>grp</th></tr></thead><tbody><tr><td>2018-12-28</td><td>failed</td><td>1</td><td>2018-12-27</td></tr><tr><td>2018-12-29</td><td>failed</td><td>2</td><td>2018-12-27</td></tr><tr><td>2019-01-04</td><td>failed</td><td>3</td><td>2019-01-01</td></tr><tr><td>2019-01-05</td><td>failed</td><td>4</td><td>2019-01-01</td></tr><tr><td>2018-12-30</td><td>succeeded</td><td>1</td><td>2018-12-29</td></tr><tr><td>2018-12-31</td><td>succeeded</td><td>2</td><td>2018-12-29</td></tr><tr><td>2019-01-01</td><td>succeeded</td><td>3</td><td>2018-12-29</td></tr><tr><td>2019-01-02</td><td>succeeded</td><td>4</td><td>2018-12-29</td></tr><tr><td>2019-01-03</td><td>succeeded</td><td>5</td><td>2018-12-29</td></tr><tr><td>2019-01-06</td><td>succeeded</td><td>6</td><td>2018-12-31</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2018-12-28",
         "failed",
         1,
         "2018-12-27"
        ],
        [
         "2018-12-29",
         "failed",
         2,
         "2018-12-27"
        ],
        [
         "2019-01-04",
         "failed",
         3,
         "2019-01-01"
        ],
        [
         "2019-01-05",
         "failed",
         4,
         "2019-01-01"
        ],
        [
         "2018-12-30",
         "succeeded",
         1,
         "2018-12-29"
        ],
        [
         "2018-12-31",
         "succeeded",
         2,
         "2018-12-29"
        ],
        [
         "2019-01-01",
         "succeeded",
         3,
         "2018-12-29"
        ],
        [
         "2019-01-02",
         "succeeded",
         4,
         "2018-12-29"
        ],
        [
         "2019-01-03",
         "succeeded",
         5,
         "2018-12-29"
        ],
        [
         "2019-01-06",
         "succeeded",
         6,
         "2018-12-31"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "task_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "period_state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rr",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "grp",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "WITH all_tasks AS (\n",
    "    SELECT \n",
    "        fail_date AS task_date, \n",
    "        'failed' AS period_state\n",
    "    FROM leet.failed\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        success_date AS task_date, \n",
    "        'succeeded' AS period_state\n",
    "    FROM leet.succeeded\n",
    "),\n",
    "grouped_tasks AS (\n",
    "    SELECT \n",
    "        task_date,\n",
    "        period_state,\n",
    "        ROW_NUMBER() OVER (PARTITION BY period_state ORDER BY task_date) rr,\n",
    "        DATE_SUB(task_date, ROW_NUMBER() OVER (PARTITION BY period_state ORDER BY task_date)) AS grp\n",
    "    FROM all_tasks\n",
    ")\n",
    "SELECT \n",
    "    *\n",
    "FROM grouped_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74849ea0-4c55-4e85-baa5-8e10a08d0c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>period_state</th><th>start_date</th><th>end_date</th></tr></thead><tbody><tr><td>succeeded</td><td>2019-01-01</td><td>2019-01-03</td></tr><tr><td>failed</td><td>2019-01-04</td><td>2019-01-05</td></tr><tr><td>succeeded</td><td>2019-01-06</td><td>2019-01-06</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "succeeded",
         "2019-01-01",
         "2019-01-03"
        ],
        [
         "failed",
         "2019-01-04",
         "2019-01-05"
        ],
        [
         "succeeded",
         "2019-01-06",
         "2019-01-06"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "period_state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "start_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "end_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "WITH Combined AS (\n",
    "    SELECT fail_date AS dt, 'failed' AS period_state\n",
    "    FROM leet.failed\n",
    "    WHERE YEAR(fail_date) = 2019\n",
    "    UNION ALL\n",
    "    SELECT success_date AS dt, 'succeeded' AS period_state\n",
    "    FROM leet.succeeded\n",
    "    WHERE YEAR(success_date) = 2019\n",
    "),\n",
    "Ranked AS (\n",
    "    SELECT\n",
    "        dt,\n",
    "        period_state,\n",
    "        row_number() OVER (PARTITION BY period_state ORDER BY dt) rr,\n",
    "        DATE_SUB(dt, row_number() OVER (PARTITION BY period_state ORDER BY dt)) AS group_key\n",
    "    FROM Combined\n",
    ")\n",
    "SELECT\n",
    "    period_state,\n",
    "    MIN(dt) AS start_date,\n",
    "    MAX(dt) AS end_date\n",
    "FROM Ranked\n",
    "GROUP BY period_state, group_key\n",
    "ORDER BY start_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59711454-106a-4382-bbee-9915e80da88d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dt</th><th>period_state</th><th>rr</th><th>group_key</th></tr></thead><tbody><tr><td>2019-01-04</td><td>failed</td><td>1</td><td>2019-01-03</td></tr><tr><td>2019-01-05</td><td>failed</td><td>2</td><td>2019-01-03</td></tr><tr><td>2019-01-01</td><td>succeeded</td><td>1</td><td>2018-12-31</td></tr><tr><td>2019-01-02</td><td>succeeded</td><td>2</td><td>2018-12-31</td></tr><tr><td>2019-01-03</td><td>succeeded</td><td>3</td><td>2018-12-31</td></tr><tr><td>2019-01-06</td><td>succeeded</td><td>4</td><td>2019-01-02</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2019-01-04",
         "failed",
         1,
         "2019-01-03"
        ],
        [
         "2019-01-05",
         "failed",
         2,
         "2019-01-03"
        ],
        [
         "2019-01-01",
         "succeeded",
         1,
         "2018-12-31"
        ],
        [
         "2019-01-02",
         "succeeded",
         2,
         "2018-12-31"
        ],
        [
         "2019-01-03",
         "succeeded",
         3,
         "2018-12-31"
        ],
        [
         "2019-01-06",
         "succeeded",
         4,
         "2019-01-02"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dt",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "period_state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rr",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "group_key",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "WITH Combined AS (\n",
    "    SELECT fail_date AS dt, 'failed' AS period_state\n",
    "    FROM leet.failed\n",
    "    WHERE YEAR(fail_date) = 2019\n",
    "    UNION ALL\n",
    "    SELECT success_date AS dt, 'succeeded' AS period_state\n",
    "    FROM leet.succeeded\n",
    "    WHERE YEAR(success_date) = 2019\n",
    "),\n",
    "Ranked AS (\n",
    "    SELECT\n",
    "        dt,\n",
    "        period_state,\n",
    "        row_number() OVER (PARTITION BY period_state ORDER BY dt) rr,\n",
    "        DATE_SUB(dt, (row_number() OVER (PARTITION BY period_state ORDER BY dt))) AS group_key\n",
    "    FROM Combined\n",
    ")\n",
    "SELECT dt, period_state, rr, group_key\n",
    "   -- period_state,\n",
    "   -- MIN(dt) AS start_date,\n",
    "   -- MAX(dt) AS end_date\n",
    "FROM Ranked\n",
    "--GROUP BY period_state, group_key\n",
    "--ORDER BY start_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db0c0ded-fb68-4c22-a0af-acc6696af978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>period_state</th><th>start_date</th><th>end_date</th></tr></thead><tbody><tr><td>succeeded</td><td>2019-01-01</td><td>2019-01-03</td></tr><tr><td>failed</td><td>2019-01-04</td><td>2019-01-05</td></tr><tr><td>succeeded</td><td>2019-01-06</td><td>2019-01-06</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "succeeded",
         "2019-01-01",
         "2019-01-03"
        ],
        [
         "failed",
         "2019-01-04",
         "2019-01-05"
        ],
        [
         "succeeded",
         "2019-01-06",
         "2019-01-06"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "period_state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "start_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "end_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "WITH Combined AS (\n",
    "    SELECT fail_date AS dt, 'failed' AS period_state\n",
    "    FROM leet.failed\n",
    "    WHERE YEAR(fail_date) = 2019\n",
    "    UNION ALL\n",
    "    SELECT success_date AS dt, 'succeeded' AS period_state\n",
    "    FROM leet.succeeded\n",
    "    WHERE YEAR(success_date) = 2019\n",
    "),\n",
    "Ranked AS (\n",
    "    SELECT\n",
    "        dt,\n",
    "        period_state,\n",
    "        RANK() OVER (PARTITION BY period_state ORDER BY dt) rr,\n",
    "        DATE_SUB(dt, RANK() OVER (PARTITION BY period_state ORDER BY dt)) AS group_key\n",
    "    FROM Combined\n",
    ")\n",
    "SELECT\n",
    "    period_state,\n",
    "    MIN(dt) AS start_date,\n",
    "    MAX(dt) AS end_date\n",
    "FROM Ranked\n",
    "GROUP BY period_state, group_key\n",
    "ORDER BY start_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c269882-ad79-4bcf-b8d4-9b06ec26a782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dt</th><th>period_state</th><th>rr</th><th>group_key</th></tr></thead><tbody><tr><td>2019-01-04</td><td>failed</td><td>1</td><td>2019-01-04</td></tr><tr><td>2019-01-05</td><td>failed</td><td>2</td><td>2019-01-04</td></tr><tr><td>2019-01-01</td><td>succeeded</td><td>1</td><td>2019-01-01</td></tr><tr><td>2019-01-02</td><td>succeeded</td><td>2</td><td>2019-01-01</td></tr><tr><td>2019-01-03</td><td>succeeded</td><td>3</td><td>2019-01-01</td></tr><tr><td>2019-01-06</td><td>succeeded</td><td>4</td><td>2019-01-03</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2019-01-04",
         "failed",
         1,
         "2019-01-04"
        ],
        [
         "2019-01-05",
         "failed",
         2,
         "2019-01-04"
        ],
        [
         "2019-01-01",
         "succeeded",
         1,
         "2019-01-01"
        ],
        [
         "2019-01-02",
         "succeeded",
         2,
         "2019-01-01"
        ],
        [
         "2019-01-03",
         "succeeded",
         3,
         "2019-01-01"
        ],
        [
         "2019-01-06",
         "succeeded",
         4,
         "2019-01-03"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dt",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "period_state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rr",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "group_key",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "WITH Combined AS (\n",
    "    SELECT fail_date AS dt, 'failed' AS period_state\n",
    "    FROM leet.failed\n",
    "    WHERE YEAR(fail_date) = 2019\n",
    "    UNION ALL\n",
    "    SELECT success_date AS dt, 'succeeded' AS period_state\n",
    "    FROM leet.succeeded\n",
    "    WHERE YEAR(success_date) = 2019\n",
    "),\n",
    "Ranked AS (\n",
    "    SELECT\n",
    "        dt,\n",
    "        period_state,\n",
    "        row_number() OVER (PARTITION BY period_state ORDER BY dt) rr,\n",
    "        DATE_SUB(dt, row_number() OVER (PARTITION BY period_state ORDER BY dt)-1) AS group_key\n",
    "    FROM Combined\n",
    ")\n",
    "SELECT\n",
    "    *\n",
    "FROM Ranked\n",
    "--GROUP BY period_state, group_key\n",
    "--ORDER BY start_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1493043-fabd-4273-8dd3-a18c48894e30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>period_state</th><th>start_date</th><th>end_date</th></tr></thead><tbody><tr><td>succeeded</td><td>2019-01-01</td><td>2019-01-03</td></tr><tr><td>failed</td><td>2019-01-04</td><td>2019-01-05</td></tr><tr><td>succeeded</td><td>2019-01-06</td><td>2019-01-06</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "succeeded",
         "2019-01-01",
         "2019-01-03"
        ],
        [
         "failed",
         "2019-01-04",
         "2019-01-05"
        ],
        [
         "succeeded",
         "2019-01-06",
         "2019-01-06"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "period_state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "start_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "end_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "WITH Combined AS (\n",
    "    SELECT fail_date AS dt, 'failed' AS period_state\n",
    "    FROM leet.failed\n",
    "    WHERE YEAR(fail_date) = 2019\n",
    "    UNION ALL\n",
    "    SELECT success_date AS dt, 'succeeded' AS period_state\n",
    "    FROM leet.succeeded\n",
    "    WHERE YEAR(success_date) = 2019\n",
    "),\n",
    "Ranked AS (\n",
    "    SELECT\n",
    "        dt,\n",
    "        period_state,\n",
    "        row_number() OVER (PARTITION BY period_state ORDER BY dt) rr,\n",
    "        DATE_SUB(dt, row_number() OVER (PARTITION BY period_state ORDER BY dt)-1) AS group_key\n",
    "    FROM Combined\n",
    ")\n",
    "SELECT\n",
    "    period_state,\n",
    "    MIN(dt) AS start_date,\n",
    "    MAX(dt) AS end_date\n",
    "FROM Ranked\n",
    "GROUP BY period_state, group_key\n",
    "ORDER BY start_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b72c3dc-2016-49de-aea5-3c091069b2fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1225 - Report Contiguous Dates",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}